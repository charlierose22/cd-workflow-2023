library(tidyverse)
library(dplyr)
library(readxl)
library(readr)
library(readr)
Raw_Data <- read_csv("Raw_Data.csv")
View(Raw_Data)
# import data
Raw_Data <- read_csv("Raw_Data.csv")
# install/load packages
library(tidyverse)
library(readxl)
library(readr)
# import data
Raw_Data <- read_csv("Raw_Data.csv")
View(Raw_Data)
# install/load packages (working within a project so working directory is fine)
library(tidyverse)
# import data from csv file, using the janitor package to clean the name format up.
Raw_Data <- readr::read_csv("~/Desktop/Charlie_CD/Charlie_CD/Raw_Data.csv") %>%
janitor::clean_names()
# drop unnecessary columns entirely, unless you have used them in the CD software.
Raw_Data$tags = NULL
Raw_Data$checked = NULL
# to make sure we don't edit any raw data files, duplicate and call the original dataset "base".
Base <- Raw_Data
# starting with the group area measurements, lengthen the table to erase white space.
# this will also clean up names so we can see which sample each compound is found in.
# the first "select" function removes the peak rating data, so we can deal with that later.
Group_Area <- select(Base, -c(starts_with("peak")))
Group_Area_Long <- pivot_longer(Group_Area,
c(starts_with("group")),
names_to = "sample",
names_prefix = "group_area_",
values_to = "group_area")
# do the same with the peak rating data, as was done with the group area measurements.
# remove the group area data first, as we have lengthened this already.
Peak_Rating <- select(Base, -c(starts_with("group")))
Peak_Rating_Long <- pivot_longer(Peak_Rating,
c(starts_with("peak")),
names_to = "sample",
names_prefix = "peak_rating_",
values_to = "peak_rating")
# clean up sample names column in the peak_rating_table, to remove the numbered file names from the CD software.
# this line uses regular expressions to tell us which part of the name to remove.
# clean up sample names column in the peak_rating_table, to remove the numbered file names from the CD software.
# this line uses regular expressions to tell us which part of the name to remove.
Peak_Rating_Long$sample <- stringi::stri_replace_all_regex(Peak_Rating_Long$sample, "^*_raw_f[\\dd]*", "")
View(Peak_Rating_Long)
View(Group_Area)
View(Base)
View(Group_Area_Long)
View(Peak_Rating_Long)
View(Peak_Rating_Long)
View(Peak_Rating)
# concatenate compound names and retention times to make a unique identifier
BaseUniqueID <- add_column(Base, "unique_id", .after = 1)
BaseUniqueID$unique_id = str_c(BaseUniqueID$name, "_", BaseUniqueID$rt_min)
View(BaseUniqueID)
# concatenate compound names and retention times to make a unique identifier
BaseUniqueID <- add_column(Base, unique_id, .after = 1)
# to make sure we don't edit any raw data files, duplicate and call the original dataset "base".
Base <- Raw_Data
# concatenate compound names and retention times to make a unique identifier
BaseUniqueID <- add_column(Base, unique_id, .after = 1)
# concatenate compound names and retention times to make a unique identifier
Base$unique_id = NA
View(Base)
as.data.frame(Base)
BaseUniqueID <- add_column(Base, unique_id, .after = 1)
# to make sure we don't edit any raw data files, duplicate and call the original dataset "base".
Base <- Raw_Data
# concatenate compound names and retention times to make a unique identifier
BaseUniqueID <- add_column(Base, unique_id = NA, .after = 1)
View(BaseUniqueID)
# to make sure we don't edit any raw data files, duplicate and call the original dataset "base".
Base <- Raw_Data
# concatenate compound names and retention times to make a unique identifier
BaseUniqueID <- add_column(Base, unique_id = NA, .after = 0)
View(BaseUniqueID)
BaseUniqueID_Fill <- unite(BaseUniqueID$unique_id, name:rt_min)
BaseUniqueID_Fill <- BaseUniqueID$unique_id <- str_c(BaseUniqueID$name, "_", BaseUniqueID$rt_min)
View(BaseUniqueID)
View(BaseUniqueID)
View(BaseUniqueID)
# add peak numer in as another unique identifier.
BaseUniqueID_PeakNumber <- add_column(BaseUniqueID, peak_number = NA, .after = 0)
View(BaseUniqueID_PeakNumber)
BaseUniqueID_PeakNumber$peak_number <- seq.int(nrow(BaseUniqueID_PeakNumber))
# starting with the group area measurements, lengthen the table to erase white space.
# this will also clean up names so we can see which sample each compound is found in.
# the first "select" function removes the peak rating data, so we can deal with that later.
#Group_Area <- select(Base, -c(starts_with("peak")))
Group_Area_Long <- pivot_longer(BaseUniqueID_PeakNumber, c(starts_with("group")),
names_to = "sample",
names_prefix = "group_area_",
values_to = "group_area")
View(Group_Area_Long)
# do the same with the peak rating data, as was done with the group area measurements.
# remove the group area data first, as we have lengthened this already.
Peak_Rating_Long <- pivot_longer(Group_Area_Long,
c(contains("rating")),
names_to = "sample_file",
names_prefix = "peak_rating_",
values_to = "peak_rating")
View(Peak_Rating_Long)
View(BaseUniqueID)
View(Peak_Rating_Long)
View(Group_Area_Long)
steve = colnames(Group_Area_Long)
View(steve)
write.csv(steve, file="col_names.txt", sep="\n")
write.csv(steve, file="col_names.txt")
write.csv(steve, file="col_names.txt", sep="\t")
write.table(steve, file="col_names.txt",)
# clean up sample names column in the peak_rating_table, to remove the numbered file names from the CD software.
# this line uses regular expressions to tell us which part of the name to remove.
Peak_Rating_Long$sample_file <- stringi::stri_replace_all_regex(Peak_Rating_Long$sample_file, "^*_raw_f[\\dd]*", "")
View(Peak_Rating_Long)
#SETUP/CLEAN----------
# install/load packages (working within a project so working directory is fine)
library(tidyverse)
# import data from csv file, using the janitor package to clean the name format up.
Raw_Data <- readr::read_csv("~/Desktop/Charlie_CD/Charlie_CD/Raw_Data.csv") %>%
janitor::clean_names()
# to make sure we don't edit any raw data files, duplicate and call the original dataset "base".
Base <- Raw_Data
View(Base)
# drop unnecessary columns entirely, unless you have used them in the CD software.
Base$tags = NULL
Base$checked = NULL
# concatenate compound names and retention times to make a unique identifier, this will make things easier later on.
BaseUniqueID <- add_column(Base, unique_id = NA, .after = 0)
BaseUniqueID$unique_id <- str_c(BaseUniqueID$name, "_", BaseUniqueID$rt_min)
View(BaseUniqueID)
# add peak number in as another unique identifier.
BaseUniqueID_PeakNumber <- add_column(BaseUniqueID, peak_number = NA, .after = 0)
BaseUniqueID_PeakNumber$peak_number <- seq.int(nrow(BaseUniqueID_PeakNumber))
View(BaseUniqueID_PeakNumber)
# starting with the group area measurements, lengthen the table to erase white space.
colnames(BaseUniqueID_PeakNumber) <- sub("*_raw_f\\d\\d*", "", colnames(BaseUniqueID_PeakNumber))
View(BaseUniqueID_PeakNumber)
Longer <- BaseUniqueID_PeakNumber %>%
pivot_longer(cols = group_area_1_feedpump_a:peak_rating_qc3,
names_to = "sample",
values_to = "result")
# create a new column for sample name
SampleNames <- add_column(Longer, measurement = NA)
View(Longer)
# create a new column for sample name
SampleNames <- add_column(Longer, measurement = NA)
# fill in sample names
SampleNames <- mutate(SampleNames,
measurement = case_when(str_detect(sample, "group_area") ~ "group_area",
str_detect(sample, "peak_rating") ~ "peak_rating"))
View(SampleNames)
SampleNames$sample <- str_replace_all(SampleNames$sample, "group_area_", "")
library(readr)
Charlie_KPS_19JAN_22 <- read_csv("~/Desktop/Charlie_CD/Charlie_CD/Raw Data/Charlie_KPS_19JAN_22.csv")
View(Charlie_KPS_19JAN_22)
#SETUP/CLEAN----------
# install/load packages (working within a project so working directory is fine)
library(tidyverse)
# import data from csv file, using the janitor package to clean the name format up.
Raw_Data <-  %>% readr::read_csv("~/Desktop/Charlie_CD/Charlie_CD/Raw Data/Charlie_KPS_19JAN_22.csv")
#SETUP/CLEAN----------
# Install/load packages (working within a project so working directory is fine)
library(tidyverse)
# Import data from csv file, using the janitor package to clean the name format up.
# Make sure you change the name of the file to match, and check the pathway!
Raw_Data <- readr::read_csv("~/Desktop/Charlie_CD/Charlie_CD/Raw Data/Charlie_KPS_19JAN_22.csv") %>%
janitor::clean_names()
#SETUP/CLEAN----------
# Install/load packages (working within a project so working directory is fine)
library(tidyverse)
# Import data from csv file, using the janitor package to clean the name format up.
# Make sure you change the name of the file to match, and check the pathway!
Raw_Data <- readr::read_csv("~/Desktop/Charlie_CD/Charlie_CD/Raw Data/Charlie_KPS_19JAN_22.csv") %>%
janitor::clean_names()
# To make sure we don't edit any raw data files, duplicate and call the original dataset "base".
Base <- Raw_Data
# drop unnecessary columns entirely, unless you have used them in the CD software.
Base$tags = NULL
Base$checked = NULL
# concatenate compound names and retention times to make a unique identifier, this will make things easier later on.
BaseUniqueID <- add_column(Base, unique_id = NA, .after = 0)
BaseUniqueID$unique_id <- str_c(BaseUniqueID$name, "_", BaseUniqueID$rt_min)
# add peak number in as another unique identifier.
BaseUniqueID_PeakNumber <- add_column(BaseUniqueID, peak_number = NA, .after = 0)
BaseUniqueID_PeakNumber$peak_number <- seq.int(nrow(BaseUniqueID_PeakNumber))
# starting with the group area measurements, lengthen the table to erase white space.
colnames(BaseUniqueID_PeakNumber) <- sub("*_raw_f\\d\\d*", "", colnames(BaseUniqueID_PeakNumber))
# pivot longer all in one
Longer <- BaseUniqueID_PeakNumber %>%
pivot_longer(cols = group_area_1_feedpump_a:peak_rating_qc3,
names_to = "sample",
values_to = "result")
# create a new column for sample name
SampleNames <- add_column(Longer, measurement = NA)
# fill in sample names
SampleNames <- mutate(SampleNames,
measurement = case_when(str_detect(sample, "group_area") ~ "group_area",
str_detect(sample, "peak_rating") ~ "peak_rating"))
# clean up sample column
SampleNames$sample <- str_replace_all(SampleNames$sample, "group_area_", "")
SampleNames$sample <- str_replace_all(SampleNames$sample, "peak_rating_", "")
# pivot wider?
Wider <- SampleNames %>%
pivot_wider(names_from = measurement, values_from = result)
# remove NAs and filter so that the peak_rating column only has values above 5.
NoNAs <- drop_na(Wider, group_area)
PeakRatingFiltered <- subset(NoNAs, peak_rating > 5)
# CHANGE THIS NUMBER DEPENDING ON INTENSITY FILTER
GroupAreaFiltered <- subset(PeakRatingFiltered, group_area > 100000)
# create column for replicate IDs.
FilteredReplicate <- add_column(GroupAreaFiltered, replicate = NA)
# fill replicate file based on end of string in sample column
FilteredReplicate <- mutate(FilteredReplicate,
replicate = case_when(
str_ends(sample, "a") ~ "1",
str_ends(sample, "b") ~ "2",
str_ends(sample, "c") ~ "3"))
# Add location column for each sample and remove numbers (DO NOT PUT NUMBERS IN LOCATION TITLES! e.g. if you're talking pipe_1/pipe_2, call them pipe_a/pipe_b)
FilteredReplicate$sample_location = FilteredReplicate$sample
FilteredReplicate$sample_location <- stringi::stri_replace_all_regex(FilteredReplicate$sample_location, "^\\d|\\d|_*", "")
FilteredReplicate$sample_location <- gsub('.{1}$', '', FilteredReplicate$sample_location)
View(FilteredReplicate)
FilteredReplicate <- add_column(GroupAreaFiltered, replicate = NA)
# fill replicate file based on end of string in sample column
FilteredReplicate <- mutate(FilteredReplicate,
replicate = case_when(
str_ends(sample, "a") ~ "1",
str_ends(sample, "b") ~ "2",
str_ends(sample, "c") ~ "3"))
# Add location column for each sample and remove numbers (DO NOT PUT NUMBERS IN LOCATION TITLES! e.g. if you're talking pipe_1/pipe_2, call them pipe_a/pipe_b)
FilteredReplicate$sample_location = FilteredReplicate$sample
FilteredReplicate$sample_location <- stringi::stri_replace_all_regex(FilteredReplicate$sample_location, "^\\d|\\d_*", "")
FilteredReplicate$sample_location <- gsub('.{1}$', '', FilteredReplicate$sample_location)
View(FilteredReplicate)
FilteredReplicate <- add_column(GroupAreaFiltered, replicate = NA)
# fill replicate file based on end of string in sample column
FilteredReplicate <- mutate(FilteredReplicate,
replicate = case_when(
str_ends(sample, "a") ~ "1",
str_ends(sample, "b") ~ "2",
str_ends(sample, "c") ~ "3"))
# Add location column for each sample and remove numbers (DO NOT PUT NUMBERS IN LOCATION TITLES! e.g. if you're talking pipe_1/pipe_2, call them pipe_a/pipe_b)
FilteredReplicate$sample_location = FilteredReplicate$sample
FilteredReplicate$sample_location <- stringi::stri_replace_all_regex(FilteredReplicate$sample_location, "^\\d|_*", "")
FilteredReplicate$sample_location <- gsub('.{1}$', '', FilteredReplicate$sample_location)
# SPECIFIC FOR THIS DATASET
# correct the digester numbers
FilteredDigesterCorrect <- add_column(FilteredReplicate, digester_number = NA)
FilteredDigesterCorrect <- mutate(FilteredDigesterCorrect,
digester_number = case_when(
str_detect(sample, "digester1") ~ "A",
str_detect(sample, "digester2") ~ "B",
str_detect(sample, "digester3") ~ "C",
str_detect(sample, "digester4") ~ "D",
))
View(FilteredReplicate)
FilteredReplicate <- add_column(GroupAreaFiltered, replicate = NA)
# fill replicate file based on end of string in sample column
FilteredReplicate <- mutate(FilteredReplicate,
replicate = case_when(
str_ends(sample, "a") ~ "1",
str_ends(sample, "b") ~ "2",
str_ends(sample, "c") ~ "3"))
# Add location column for each sample and remove numbers (DO NOT PUT NUMBERS IN LOCATION TITLES! e.g. if you're talking pipe_1/pipe_2, call them pipe_a/pipe_b)
FilteredReplicate$sample_location = FilteredReplicate$sample
FilteredReplicate$sample_location <- stringi::stri_replace_all_regex(FilteredReplicate$sample_location, "^\\d|\\d|_*", "")
FilteredReplicate$sample_location <- gsub('.{1}$', '', FilteredReplicate$sample_location)
# SPECIFIC FOR THIS DATASET
# correct the digester numbers
FilteredDigesterCorrect <- add_column(FilteredReplicate, digester_number = NA)
FilteredDigesterCorrect <- mutate(FilteredDigesterCorrect,
digester_number = case_when(
str_detect(sample, "digester1") ~ "A",
str_detect(sample, "digester2") ~ "B",
str_detect(sample, "digester3") ~ "C",
str_detect(sample, "digester4") ~ "D",
))
View(FilteredReplicate)
View(FilteredDigesterCorrect)
SoloRemoved <- plyr::ddply(FilteredReplicate, c("unique_id", "sample_location"),
function(d) {if (nrow(d) > 1) d else NULL})
View(SoloRemoved)
# Split by the mass_list_search column, and make two tables for mzcloud results and mass_list results
Split <- split(SoloRemoved, SoloRemoved$annot_source_mass_list_search)
View(Split)
MZCloud <- Split$"No results"
MassList < Split$"Full match"
MassList <- Split$"Full match"
View(MassList)
# Bring together the mass lists so we can split by specific mass list.
MassListLonger <- MassList %>%
pivot_longer(cols = c(starts_with("mass_list_match")) ,
names_to = "mass_list_name",
prefix = "mass_list_match_",
values_to = "mass_list_match")
MassListLonger <- MassList %>%
pivot_longer(cols = c(starts_with("mass_list_match")) ,
names_to = "mass_list_name",
names_prefix = "mass_list_match_",
values_to = "mass_list_match")
View(MassListLonger)
View(FilteredReplicate)
# Remove "solo" results.
SoloRemoved <- plyr::ddply(FilteredDigesterCorrect, c("unique_id", "sample_location"),
function(d) {if (nrow(d) > 1) d else NULL})
# Split by the mass_list_search column, and make two tables for mzcloud results and mass_list results
Split <- split(SoloRemoved, SoloRemoved$annot_source_mass_list_search)
MZCloud <- Split$"No results"
MassList <- Split$"Full match"
# Bring together the mass lists so we can split by specific mass list.
MassListLonger <- MassList %>%
pivot_longer(cols = c(starts_with("mass_list_match")) ,
names_to = "mass_list_name",
names_prefix = "mass_list_match_",
values_to = "mass_list_match")
#SETUP/CLEAN----------
# Install/load packages (working within a project so working directory is fine)
library(tidyverse)
# Import data from csv file, using the janitor package to clean the name format up.
# Make sure you change the name of the file to match, and check the pathway!
Raw_Data <- readr::read_csv("~/Desktop/Charlie_CD/Charlie_CD/Raw Data/Charlie_KPS_19JAN_22.csv") %>%
janitor::clean_names()
# To make sure we don't edit any raw data files, duplicate and call the original dataset "base".
Base <- Raw_Data
# drop unnecessary columns entirely, unless you have used them in the CD software.
Base$tags = NULL
Base$checked = NULL
# concatenate compound names and retention times to make a unique identifier, this will make things easier later on.
BaseUniqueID <- add_column(Base, unique_id = NA, .after = 0)
BaseUniqueID$unique_id <- str_c(BaseUniqueID$name, "_", BaseUniqueID$rt_min)
# add peak number in as another unique identifier.
BaseUniqueID_PeakNumber <- add_column(BaseUniqueID, peak_number = NA, .after = 0)
BaseUniqueID_PeakNumber$peak_number <- seq.int(nrow(BaseUniqueID_PeakNumber))
# starting with the group area measurements, lengthen the table to erase white space.
colnames(BaseUniqueID_PeakNumber) <- sub("*_raw_f\\d\\d*", "", colnames(BaseUniqueID_PeakNumber))
# pivot longer all in one
Longer <- BaseUniqueID_PeakNumber %>%
pivot_longer(cols = group_area_1_feedpump_a:peak_rating_qc3,
names_to = "sample",
values_to = "result")
# create a new column for sample name
SampleNames <- add_column(Longer, measurement = NA)
# fill in sample names
SampleNames <- mutate(SampleNames,
measurement = case_when(str_detect(sample, "group_area") ~ "group_area",
str_detect(sample, "peak_rating") ~ "peak_rating"))
# clean up sample column
SampleNames$sample <- str_replace_all(SampleNames$sample, "group_area_", "")
SampleNames$sample <- str_replace_all(SampleNames$sample, "peak_rating_", "")
# pivot wider?
Wider <- SampleNames %>%
pivot_wider(names_from = measurement, values_from = result)
#FILTER----------
# remove NAs and filter so that the peak_rating column only has values above 5.
NoNAs <- drop_na(Wider, group_area)
PeakRatingFiltered <- subset(NoNAs, peak_rating > 5)
# CHANGE THIS NUMBER DEPENDING ON INTENSITY FILTER
GroupAreaFiltered <- subset(PeakRatingFiltered, group_area > 100000)
# create column for replicate IDs.
FilteredReplicate <- add_column(GroupAreaFiltered, replicate = NA)
# fill replicate file based on end of string in sample column
FilteredReplicate <- mutate(FilteredReplicate,
replicate = case_when(
str_ends(sample, "a") ~ "1",
str_ends(sample, "b") ~ "2",
str_ends(sample, "c") ~ "3"))
# Add location column for each sample and remove numbers (DO NOT PUT NUMBERS IN LOCATION TITLES! e.g. if you're talking pipe_1/pipe_2, call them pipe_a/pipe_b)
FilteredReplicate$sample_location = FilteredReplicate$sample
FilteredReplicate$sample_location <- stringi::stri_replace_all_regex(FilteredReplicate$sample_location, "^\\d|\\d|_*", "")
FilteredReplicate$sample_location <- gsub('.{1}$', '', FilteredReplicate$sample_location)
# SPECIFIC FOR THIS DATASET
# correct the digester numbers (or correct anything that has number separation)
FilteredDigesterCorrect <- add_column(FilteredReplicate, digester_number = NA)
FilteredDigesterCorrect <- mutate(FilteredDigesterCorrect,
digester_number = case_when(
str_detect(sample, "digester1") ~ "A",
str_detect(sample, "digester2") ~ "B",
str_detect(sample, "digester3") ~ "C",
str_detect(sample, "digester4") ~ "D",
))
# Remove "solo" results.
SoloRemoved <- plyr::ddply(FilteredDigesterCorrect, c("unique_id", "sample_location"),
function(d) {if (nrow(d) > 1) d else NULL})
# Split by the mass_list_search column, and make two tables for mzcloud results and mass_list results
Split <- split(SoloRemoved, SoloRemoved$annot_source_mass_list_search)
MZCloud <- Split$"No results"
MassList <- Split$"Full match"
# Bring together the mass lists so we can split by specific mass list.
MassListLonger <- MassList %>%
pivot_longer(cols = c(starts_with("mass_list_match")) ,
names_to = "mass_list_name",
names_prefix = "mass_list_match_",
values_to = "mass_list_match")
GroupAreaFiltered <- subset(PeakRatingFiltered, group_area > 1000000)
# create column for replicate IDs.
FilteredReplicate <- add_column(GroupAreaFiltered, replicate = NA)
# fill replicate file based on end of string in sample column
FilteredReplicate <- mutate(FilteredReplicate,
replicate = case_when(
str_ends(sample, "a") ~ "1",
str_ends(sample, "b") ~ "2",
str_ends(sample, "c") ~ "3"))
# Add location column for each sample and remove numbers (DO NOT PUT NUMBERS IN LOCATION TITLES! e.g. if you're talking pipe_1/pipe_2, call them pipe_a/pipe_b)
FilteredReplicate$sample_location = FilteredReplicate$sample
FilteredReplicate$sample_location <- stringi::stri_replace_all_regex(FilteredReplicate$sample_location, "^\\d|\\d|_*", "")
FilteredReplicate$sample_location <- gsub('.{1}$', '', FilteredReplicate$sample_location)
# SPECIFIC FOR THIS DATASET
# correct the digester numbers (or correct anything that has number separation)
FilteredDigesterCorrect <- add_column(FilteredReplicate, digester_number = NA)
FilteredDigesterCorrect <- mutate(FilteredDigesterCorrect,
digester_number = case_when(
str_detect(sample, "digester1") ~ "A",
str_detect(sample, "digester2") ~ "B",
str_detect(sample, "digester3") ~ "C",
str_detect(sample, "digester4") ~ "D",
))
# Remove "solo" results.
SoloRemoved <- plyr::ddply(FilteredDigesterCorrect, c("unique_id", "sample_location"),
function(d) {if (nrow(d) > 1) d else NULL})
# Split by the mass_list_search column, and make two tables for mzcloud results and mass_list results
Split <- split(SoloRemoved, SoloRemoved$annot_source_mass_list_search)
MZCloud <- Split$"No results"
MassList <- Split$"Full match"
# Bring together the mass lists so we can split by specific mass list.
MassListLonger <- MassList %>%
pivot_longer(cols = c(starts_with("mass_list_match")) ,
names_to = "mass_list_name",
names_prefix = "mass_list_match_",
values_to = "mass_list_match")
# filter for no matches in mass list.
View(MassListLonger)
# filter for no matches in mass list.
FilteredMassList <- MassListLonger[!grepl('No matches found', MassListLonger$mass_list_match),]
View(FilteredMassList)
NoNAs <- drop_na(Wider, group_area)
PeakRatingFiltered <- subset(NoNAs, peak_rating > 5)
# CHANGE THIS NUMBER DEPENDING ON INTENSITY FILTER
GroupAreaFiltered <- subset(PeakRatingFiltered, group_area > 100000)
# create column for replicate IDs.
FilteredReplicate <- add_column(GroupAreaFiltered, replicate = NA)
# fill replicate file based on end of string in sample column
FilteredReplicate <- mutate(FilteredReplicate,
replicate = case_when(
str_ends(sample, "a") ~ "1",
str_ends(sample, "b") ~ "2",
str_ends(sample, "c") ~ "3"))
# Add location column for each sample and remove numbers (DO NOT PUT NUMBERS IN LOCATION TITLES! e.g. if you're talking pipe_1/pipe_2, call them pipe_a/pipe_b)
FilteredReplicate$sample_location = FilteredReplicate$sample
FilteredReplicate$sample_location <- stringi::stri_replace_all_regex(FilteredReplicate$sample_location, "^\\d|\\d|_*", "")
FilteredReplicate$sample_location <- gsub('.{1}$', '', FilteredReplicate$sample_location)
# SPECIFIC FOR THIS DATASET
# correct the digester numbers (or correct anything that has number separation)
FilteredDigesterCorrect <- add_column(FilteredReplicate, digester_number = NA)
FilteredDigesterCorrect <- mutate(FilteredDigesterCorrect,
digester_number = case_when(
str_detect(sample, "digester1") ~ "A",
str_detect(sample, "digester2") ~ "B",
str_detect(sample, "digester3") ~ "C",
str_detect(sample, "digester4") ~ "D",
))
# Remove "solo" results.
SoloRemoved <- plyr::ddply(FilteredDigesterCorrect, c("unique_id", "sample_location"),
function(d) {if (nrow(d) > 1) d else NULL})
# Split by the mass_list_search column, and make two tables for mzcloud results and mass_list results
Split <- split(SoloRemoved, SoloRemoved$annot_source_mass_list_search)
MZCloud <- Split$"No results"
MassList <- Split$"Full match"
# Bring together the mass lists so we can split by specific mass list.
MassListLonger <- MassList %>%
pivot_longer(cols = c(starts_with("mass_list_match")) ,
names_to = "mass_list_name",
names_prefix = "mass_list_match_",
values_to = "mass_list_match")
# filter for no matches in mass list.
FilteredMassList <- MassListLonger[!grepl('No matches found', MassListLonger$mass_list_match),]
View(FilteredMassList)
View(FilteredMassList)
View(FilteredMassList)
# split further into mass lists
SplitMassList <- split(FilteredMassList, FilteredMassList$mass_list_name)
View(SplitMassList)
ITN <- Split$"itn_kps"
Cannabinoids <- Split$"kps_cannabinoids"
ITNMetabolites <- Split$"itn_cyp_metabolites"
# split further into mass lists
SplitMassList <- split(FilteredMassList, FilteredMassList$mass_list_name)
ITN <- Split$"itn_kps"
Cannabinoids <- Split$"kps_cannabinoids"
ITNMetabolites <- Split$"itn_cyp_metabolites"
SplitMassList <- split(FilteredMassList, FilteredMassList$mass_list_name)
ITN <- SplitMassList$"itn_kps"
Cannabinoids <- SplitMassList$"kps_cannabinoids"
ITNMetabolites <- SplitMassList$"itn_cyp_metabolites"
View(Cannabinoids)
# count unique compounds in each
length(unique(ITN$unique_id))
# count unique compounds in each
length(unique(Cannabinoids$unique_id))
# count unique compounds in each
length(unique(ITNMetabolites$unique_id))
View(ITN)
View(MZCloud)
# only keep full match to mzcloud results
FilteredMZCloud <- MZCloud[grepl('Full Match', MZCloud$annot_source_mz_cloud_search),]
View(FilteredMZCloud)
View(MZCloud)
View(ITN)
